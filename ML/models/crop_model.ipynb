# Crop Recommendation System
# This program analyzes crop data and builds models to recommend suitable crops based on soil and climate conditions

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

# Set visualization styles
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("viridis")

def load_and_explore_data(filepath):
    """
    Load and perform initial exploration of the crop dataset
    
    Args:
        filepath: Path to the CSV file
        
    Returns:
        DataFrame: The processed dataframe
    """
    # Load data
    crop = pd.read_csv(filepath)
    
    # Basic exploration
    print("Dataset Shape:", crop.shape)
    print("\nFirst 5 records:")
    print(crop.head())
    
    # Check for missing values and duplicates
    print("\nMissing values:", crop.isnull().sum().sum())
    print("Duplicate rows:", crop.duplicated().sum())
    
    # Summary statistics
    print("\nSummary Statistics:")
    print(crop.describe())
    
    # Display crop distribution
    print("\nCrop Distribution:")
    crop_counts = crop['label'].value_counts()
    print(crop_counts)
    
    # Plot crop distribution
    plt.figure(figsize=(12, 6))
    sns.countplot(y='label', data=crop, order=crop_counts.index)
    plt.title('Distribution of Crop Types')
    plt.tight_layout()
    plt.show()
    
    return crop

def analyze_correlations(crop_df):
    """
    Analyze and visualize feature correlations
    
    Args:
        crop_df: The crop dataframe
    """
    # Create one-hot encoded version for correlation analysis
    crop_encoded = pd.get_dummies(crop_df, columns=['label'], drop_first=False)
    
    # Calculate correlation
    corr = crop_encoded.iloc[:, :7].corr()  # Only numerical features
    
    # Plot correlation heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
    plt.title('Feature Correlation Matrix')
    plt.tight_layout()
    plt.show()
    
    # Find and print strongest correlations
    print("\nStrongest Feature Correlations:")
    corr_pairs = corr.unstack().sort_values(ascending=False)
    print(corr_pairs[corr_pairs < 1.0].head(10))  # Excluding self-correlations

def preprocess_data(crop_df):
    """
    Preprocess the data for modeling
    
    Args:
        crop_df: The crop dataframe
        
    Returns:
        tuple: X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, crop_mapping
    """
    # Create numeric encoding for crops
    unique_crops = crop_df['label'].unique()
    crop_mapping = {crop: idx+1 for idx, crop in enumerate(sorted(unique_crops))}
    reverse_mapping = {v: k for k, v in crop_mapping.items()}
    
    # Apply mapping
    crop_df['label_num'] = crop_df['label'].map(crop_mapping)
    
    # Split features and target
    X = crop_df.drop(['label', 'label_num'], axis=1)
    y = crop_df['label_num']
    
    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
    
    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"\nData split: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples")
    
    return X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, crop_mapping, scaler, reverse_mapping

def train_and_evaluate_models(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled):
    """
    Train and evaluate multiple ML models
    
    Args:
        X_train, X_test, y_train, y_test: Train and test data
        X_train_scaled, X_test_scaled: Standardized data
        
    Returns:
        dict: Trained models
        pd.DataFrame: Model performance summary
    """
    # Initialize models
    models = {
        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5, weights='distance'),
        'Support Vector Machine': SVC(probability=True, kernel='rbf', C=10, gamma='scale'),
        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)
    }
    
    # Initialize results storage
    results = []
    
    # Cross-validation strategy
    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # Loop through models
    for name, model in models.items():
        print(f"\n{'='*50}\nTraining {name}...\n{'='*50}")
        
        # Use scaled data for KNN and SVM, original data for tree-based models
        if name in ['K-Nearest Neighbors', 'Support Vector Machine']:
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=kf, scoring='accuracy')
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')
        
        # Calculate accuracy
        acc = accuracy_score(y_test, y_pred)
        
        # Print results
        print(f'Test Accuracy: {acc:.4f}')
        print(f'Cross-Validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}')
        
        # Add to results
        results.append([name, acc, cv_scores.mean(), cv_scores.std()])
    
    # Convert results to DataFrame
    df_results = pd.DataFrame(results, columns=["Model", "Test Accuracy", "Mean CV Accuracy", "CV Std Dev"])
    
    # Display and save results
    print("\nFinal Model Performance Summary:")
    print(df_results)
    df_results.to_csv("model_performance_summary.csv", index=False)
    
    return models, df_results

def plot_confusion_matrix(y_test, y_pred, class_mapping):
    """
    Plot confusion matrix for model evaluation
    
    Args:
        y_test: True labels
        y_pred: Predicted labels
        class_mapping: Mapping from numeric to string labels
    """
    # Get class labels
    class_labels = [class_mapping[i] for i in sorted(class_mapping.keys())]
    
    # Create confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    
    # Plot
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()
    
    # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=class_labels))

def plot_feature_importance(model, feature_names):
    """
    Plot feature importance for tree-based models
    
    Args:
        model: Trained model with feature_importances_ attribute
        feature_names: List of feature names
    """
    if hasattr(model, 'feature_importances_'):
        # Get feature importances
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        # Plot
        plt.figure(figsize=(10, 6))
        plt.bar(range(len(importances)), importances[indices], align='center')
        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)
        plt.xlabel('Features')
        plt.ylabel('Importance')
        plt.title('Feature Importance')
        plt.tight_layout()
        plt.show()
        
        # Print feature ranking
        print("\nFeature ranking:")
        for i, idx in enumerate(indices):
            print(f"{i+1}. {feature_names[idx]} ({importances[idx]:.4f})")

def predict_crop(N, P, K, temperature, humidity, pH, rainfall, model, crop_mapping, scaler, model_name):
    """
    Predict suitable crop based on input parameters
    
    Args:
        N, P, K: Nitrogen, Phosphorus, Potassium values
        temperature, humidity, pH, rainfall: Climate/soil parameters
        model: Trained model
        crop_mapping: Dictionary mapping numeric predictions to crop names
        scaler: Fitted scaler for feature standardization
        model_name: Name of the model being used
        
    Returns:
        tuple: Predicted crop name and confidence score
    """
    try:
        # Format input
        input_values = np.array([[N, P, K, temperature, humidity, pH, rainfall]])
        
        # Scale input if needed
        if model_name in ['K-Nearest Neighbors', 'Support Vector Machine']:
            input_scaled = scaler.transform(input_values)
            prediction = model.predict(input_scaled)
            probabilities = model.predict_proba(input_scaled).max(axis=1)
        else:
            prediction = model.predict(input_values)
            probabilities = model.predict_proba(input_values).max(axis=1)
        
        # Get crop name from mapping
        pred_crop = crop_mapping.get(prediction[0], "Unknown Crop")
        confidence = probabilities[0]
        
        # Print result
        print(f"\n**Best Crop Recommendation:** {pred_crop}")
        print(f"ðŸ”¹ Confidence Level: {confidence:.2%}")
        
        # Get alternative recommendations (top 3)
        if hasattr(model, "predict_proba"):
            if model_name in ['K-Nearest Neighbors', 'Support Vector Machine']:
                probs = model.predict_proba(input_scaled)[0]
            else:
                probs = model.predict_proba(input_values)[0]
            
            # Get top 3 alternatives
            top_indices = probs.argsort()[::-1][:3]
            print("\nAlternative Recommendations:")
            for i, idx in enumerate(top_indices):
                if i > 0:  # Skip the top one as it's already shown
                    crop_num = idx + 1  # Adjust index to match crop_mapping
                    print(f"{i}. {crop_mapping.get(crop_num, 'Unknown')}: {probs[idx]:.2%}")
        
        return pred_crop, confidence
    
    except Exception as e:
        print("Error in processing input:", str(e))
        return None, None

def plot_nutrient_requirements(crop_df, crop_name=None):
    """
    Plot average nutrient requirements for crops
    
    Args:
        crop_df: The crop dataframe
        crop_name: Optional specific crop to highlight
    """
    # Calculate mean values for each nutrient by crop
    nutrient_avg = crop_df.groupby('label')[['N', 'P', 'K']].mean().reset_index()
    
    # Plot
    plt.figure(figsize=(12, 8))
    
    # Set positioning
    bar_width = 0.25
    r1 = np.arange(len(nutrient_avg))
    r2 = [x + bar_width for x in r1]
    r3 = [x + bar_width for x in r2]
    
    # Create bars
    plt.bar(r1, nutrient_avg['N'], width=bar_width, label='Nitrogen (N)')
    plt.bar(r2, nutrient_avg['P'], width=bar_width, label='Phosphorus (P)')
    plt.bar(r3, nutrient_avg['K'], width=bar_width, label='Potassium (K)')
    
    # Highlight specific crop if provided
    if crop_name and crop_name in nutrient_avg['label'].values:
        crop_idx = nutrient_avg[nutrient_avg['label'] == crop_name].index[0]
        plt.axvline(x=r2[crop_idx], color='red', linestyle='--', alpha=0.7)
        plt.text(r2[crop_idx], max(nutrient_avg[['N', 'P', 'K']].max())*1.05, 
                f"Selected: {crop_name}", color='red', ha='center')
    
    # Add labels and legend
    plt.xlabel('Crop Type')
    plt.ylabel('Average Value (kg/ha)')
    plt.title('NPK Requirements by Crop Type')
    plt.xticks([r + bar_width for r in range(len(nutrient_avg))], nutrient_avg['label'], rotation=90)
    plt.legend()
    plt.tight_layout()
    plt.show()

def main():
    """Main function to run the crop recommendation system"""
    # Step 1: Load and explore data
    crop_df = load_and_explore_data('Crop_recommendation.csv')
    
    # Step 2: Analyze correlations
    analyze_correlations(crop_df)
    
    # Step 3: Preprocess data
    X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, crop_mapping, scaler, reverse_mapping = preprocess_data(crop_df)
    
    # Step 4: Train and evaluate models
    models, results = train_and_evaluate_models(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled)
    
    # Step 5: Plot confusion matrix for best model (assuming Random Forest is best)
    best_model = models['Random Forest']
    y_pred = best_model.predict(X_test)
    plot_confusion_matrix(y_test, y_pred, reverse_mapping)
    
    # Step 6: Plot feature importance
    plot_feature_importance(best_model, X_train.columns)
    
    # Step 7: Create an interactive prediction function
    print("\n===== Crop Recommendation System =====")
    print("Enter soil and climate parameters:")
    
    # Example values (you could replace with input() for interactive usage)
    N = 90
    P = 42
    K = 43
    temp = 20
    humidity = 80
    ph = 7
    rainfall = 200
    
    # Make prediction
    pred_crop, confidence = predict_crop(
        N, P, K, temp, humidity, ph, rainfall, 
        models['Random Forest'], reverse_mapping, scaler, 'Random Forest'
    )
    
    # Plot nutrient requirements with highlighted prediction
    if pred_crop:
        plot_nutrient_requirements(crop_df, pred_crop)

if __name__ == "__main__":
    main()